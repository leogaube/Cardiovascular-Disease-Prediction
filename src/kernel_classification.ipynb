{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cardiovascular disease Prediction\n",
    "\n",
    "Cardiovascular diseases are the No.1 reason for all deaths in the world [[WHO](https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds))].\n",
    "\n",
    "There are many subgroups of different kind of diseases centred around the heart and the blood vessels:\n",
    "\n",
    "- coronary heart disease – disease of the blood vessels supplying the heart muscle\n",
    "- cerebrovascular disease – disease of the blood vessels supplying the brain\n",
    "- peripheral arterial disease – disease of blood vessels supplying the arms and legs\n",
    "- rheumatic heart disease – damage to the heart muscle and heart valves from rheumatic fever, caused by streptococcal bacteria\n",
    "- congenital heart disease – malformations of heart structure existing at birth\n",
    "- deep vein thrombosis and pulmonary embolism – blood clots in the leg veins, which can dislodge and move to the heart and lungs.\n",
    "\n",
    "The typical risk factors of Cardiovascular diseases are:\n",
    "\n",
    "- Behavioral:\n",
    "    - diet,\n",
    "    - physical inactivity,\n",
    "    - tobacco use and\n",
    "    - harmful use of alcohol.\n",
    "\n",
    "- Medical Values:\n",
    "    - raised blood pressure,\n",
    "    - raised blood glucose,\n",
    "    - raised blood lipids and\n",
    "    - overweight & obesity\n",
    "\n",
    "\n",
    "The risk factors mentioned above are quite numerous, and a doctor is only so good at asking questions and data control.\n",
    "Our motivation for this project is to help doctors and patients by providing a tool that is able to make a prediction of how likely it is that\n",
    "one will be subjected to a cardiovascular disease."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "To make this prediction possible we found a dataset containing said risk factors as features, and the label based on the Cardiovascular disease status.\n",
    "This dataset is taken from Kaggle.com and can be found [here](https://www.kaggle.com/sulianova/cardiovascular-disease-dataset).\n",
    "\n",
    "The dataset has 70 000 rows and 13 columns. There are 3 types of input features:\n",
    "\n",
    "* Objective: factual information\n",
    "* Examination: results of medical examination\n",
    "* Subjective: information given by the patient\n",
    "\n",
    "| [0] id| [1] age| [2] gender| [3] height| [4] weight|\n",
    "| ---| ---| ---| ---| ---|\n",
    "| int| int| 1 or 2 | int| float|\n",
    "| -| days| categorical code (2=men)| cm| kg |\n",
    "| -| Objective| Objective| Objective| Objective |\n",
    "\n",
    "| [5] ap_hi| [6] ap_lo| [7] cholesterol| [8] gluc|\n",
    "| ---| ---| ---| ---|\n",
    "| int| int| 1, 2, 3 | 1, 2, 3 |\n",
    "| -| -| normal, above normal, well above normal| normal, above normal, well above normal|\n",
    "| Examination| Examination| Examination| Examination|\n",
    "\n",
    ">Note: ap_hi = Systolic blood pressure, ap_lo = Diastolic blood pressure, gluc = Glucose\n",
    "\n",
    "| [9] smoke| [10] alco| [11] active| [12] cardio|\n",
    "| ---| ---| ---| ---|\n",
    "| binary| binary| binary| binary |\n",
    "| -| -| -| categorical code|\n",
    "| Subjective| Subjective| Subjective| Target|\n",
    "\n",
    ">Note: alco = Alcohol intake\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA - Data Correlation\n",
    "\n",
    "A good start for choosing the correct prediction algorithm is an exploratory data analysis (EDA).\n",
    "This analysis looks at the data and shows potential correlations or problems.\n",
    "\n",
    "More information and details can be found in this [report](../docs/CardioVascular-Diseas-Prediction.html).\n",
    "\n",
    " - Our results show that some data is corrupt a needs to be fixed (mostly blood pressure and outliers).\n",
    " - The data is not linear separable, so we definitely need a good feature function or kernel.\n",
    " - The result of the prediction should be a label and not a value, so we want a classifier.\n",
    "\n",
    "\n",
    "Based on the these results a good choice for a prediction algorithm in our project is the Kernel Logistic Regression.\n",
    "\n",
    "\n",
    "When we compare our data with the WHO's [cardiovascular risk charts](https://www.who.int/news/item/02-09-2019-who-updates-cardiovascular-risk-charts)\n",
    "we should see a trend that show's that either being a man, a smoker, a person with diabetes, a person with higher cholesterol levels,\n",
    "or an older person the more at risk one is of having a cardiovascular disease.\n",
    "\n",
    "When we look at our data we clearly see this trend:\n",
    "\n",
    "TODO vinc: copy paste Jans code\n",
    "\n",
    "- age, weight, cholesterol, glucose levels and being physical active supporting the theory\n",
    "- gender showing no clear trend\n",
    "- being a smoker and drinking alcohol showing an opposite trend"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel Logistic Regression\n",
    "\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kernel / Feature function\n",
    "\n",
    "For the best result we want to include all of our risk factors. In other words, we want a Feature Function with all our features.\n",
    "Based on our EDA we can say that our data is not linear separable. There might a case our where data is linear separable,\n",
    "but instead of finding the best feature function or introducing an extra dimension to make the data linear separable,\n",
    "we kernelize our feature function.\n",
    "\n",
    "$h(x) = w^T * \\Phi(x_i)= w^T*K(x,z) = w^T * (x^T*z+1)^d $\n",
    "\n",
    "$ min J(w) = \\frac{1}{m} \\sum_{i=1}^{m} l(h(x_i), y_i) + \\Omega(w)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "IMPORTING DATA\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-'*30); print(\"IMPORTING DATA\");print('-'*30)\n",
    "# limit dataset to 5000 instances for testing purposes (memory issues)\n",
    "df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')[:1000]\n",
    "# df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Our dataset includes corrupt values and has outliers that need to be expelled before we can make a solid prediction.\n",
    "The information provided by the dataset depends on the category. While some are binary values are\n",
    "numerical on a scale like weight and height.\n",
    "To reduce noise between the features we need to normalize these data points to a similar scale, this process is called feature scaling.\n",
    "\n",
    " TODO @Jan: wollen wir noch mehr beschreiben, z.B. siehe unten?\n",
    "\n",
    "from wiki:\n",
    "```\n",
    "For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.[1]\n",
    "\n",
    "It's also important to apply feature scaling if regularization is used as part of the loss function (so that coefficients are penalized appropriately).\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import statistics\n",
    "# TODO LEO: outlier out , define valid range, delete faulty rows\n",
    "\n",
    "# data will be saved in extra file so we dont need to run this every time\n",
    "feature_scale_flag = True\n",
    "\n",
    "if feature_scale_flag:\n",
    "    # Min/Max Scaling on I=[0,1]: x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "    # df['age_scaled'] = ((df['age'])-min(df['age']))/(max(df['age'])-min(df['age']))\n",
    "    # df['height_scaled'] = ((df['height'])-min(df['height']))/(max(df['height'])-min(df['height']))\n",
    "    # df['weight_scaled'] = ((df['weight'])-min(df['weight']))/(max(df['weight'])-min(df['weight']))\n",
    "    # Standardization: x_standardized = (x - µ) / sigma\n",
    "    df['age_standardized'] = (df['age']-statistics.mean(df['age'])) / statistics.stdev(df['age'])\n",
    "    df['height_standardized'] = (df['height']-statistics.mean(df['height'])) / statistics.stdev(df['height'])\n",
    "    df['weight_standardized'] = (df['weight']-statistics.mean(df['weight'])) / statistics.stdev(df['weight'])\n",
    "\n",
    "    # df['bmi'] = (df['weight'] / ((df['height'] / 100) ** 2)).round(decimals=2)\n",
    "    # df['bmi_high'] = (df['bmi'] >= 30).astype(int)\n",
    "\n",
    "    df['cardio'] = df['cardio'].apply(lambda t: 1 if t==1 else -1).values\n",
    "\n",
    "    # eliminate corrupted Data\n",
    "    df['ap_lo_fixed'] = [x if 50 < x < 150 else -1 for x in df['ap_lo']]\n",
    "    df['ap_hi_fixed'] = [x if 100 < x < 190 else -1 for x in df['ap_hi']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement functions\n",
    "\n",
    "### squared exponential kernel $k(x,z)$\n",
    "$k(x,z) = exp(− x^Tx−2x^Tz+z^Tz/ 2σ^2) = exp(sqdist(x,z)/2σ^2)$\n",
    "\n",
    "### hypothesis function $h(x)$\n",
    "$h_\\alpha(x) = \\alpha K = \\sum_{j=1}^{m} \\alpha_j k(x_j,x)$\n",
    "\n",
    "### loss function $l(h(x),y)$\n",
    "\n",
    "logistic loss:\n",
    "$l_{logistic}(h_\\alpha(x), y) = log(1 + e^{−y·h(x)})= log(1 + exp(−y · h_\\alpha(x)))$\n",
    "\n",
    "### $l_2$ regularizer\n",
    "\n",
    "$r = \\lambda l_2 = \\lambda\\alpha^{\\intercal}K\\alpha$\n",
    "\n",
    "\n",
    "### objective function J\n",
    "\n",
    "  kernlized logistic regression\n",
    "\n",
    "reuslting in a regularized kernlized logistic:\n",
    "$\n",
    "J(\\alpha) = \\frac{1}{m}\\sum_{i=1}^m  \\log \\big(1 + \\exp\\big(-y_i \\cdot \\sum_{j=1}^{m} \\alpha_j k(x_j,x_i)\\big) \\big) + \\lambda \\alpha^{\\intercal}K\\alpha\n",
    "$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def sqdist(X, Z):\n",
    "    p1 = np.sum(X**2, axis=1)[:, np.newaxis]\n",
    "    p2 = np.sum(Z**2, axis=1)\n",
    "    p3 = -2 * np.dot(X, Z.T)\n",
    "    return p1+p2+p3\n",
    "\n",
    "def sq_exp(X, Z, sigma):\n",
    "    return np.exp(-sqdist(X, Z)/(2*sigma**2) )\n",
    "\n",
    "\n",
    "def J(α, X, y, sigma, lam):\n",
    "    K = sq_exp(X, X, sigma)\n",
    "    m = X.shape[0]\n",
    "    total_loss = 0\n",
    "    regularization = lam * np.dot(np.dot(np.transpose(α), K), α)\n",
    "\n",
    "    for i in range(m):\n",
    "        prediction = 0\n",
    "        for j in range(m):\n",
    "            prediction += α[j]*K[i][j]\n",
    "        logistic_loss = np.log(1 + np.exp(-y[i] * prediction))\n",
    "        total_loss += logistic_loss\n",
    "\n",
    "    mean_loss = total_loss / m  + regularization\n",
    "    return mean_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement the gradient of the regularized kernlized logistic regression objective."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def dJ(α, X, y, sigma, lam):\n",
    "    K = sq_exp(X, X, sigma)\n",
    "    m = X.shape[0]\n",
    "    gradient = 0\n",
    "    regularization = 2*lam * np.dot( K, α)\n",
    "\n",
    "    for i in range(m):\n",
    "        prediction = 0\n",
    "        for j in range(m):\n",
    "            prediction += α[j]*K[i][j]\n",
    "\n",
    "        numerator = -y[i] * K[i]\n",
    "        denominator = 1 + np.exp(y[i] * prediction)\n",
    "        gradient += numerator / denominator\n",
    "\n",
    "    mean_gradient = gradient / m + regularization\n",
    "    return mean_gradient\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def kernel_lr(X, y, sigma, lam):\n",
    "    # implementation of kernel ridge regression using the scipy optimizer gradient descent\n",
    "    α = np.zeros(X.shape[0],)\n",
    "    α = minimize(J, α, args=(X, y, sigma, lam), jac=dJ, method='CG').x\n",
    "    h = lambda Z: np.dot(α, sq_exp(X, Z, sigma))\n",
    "    return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "## Split data in Train / Validation / Test\n",
    "\n",
    "We'll first split our data into a Train set (70%) and Test set (30%).  \n",
    "The training set will be further processed using 10-fold-cross-validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_proportion=0.7, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(data.shape[0])\n",
    "    else:\n",
    "        indices = np.arange(data.shape[0])\n",
    "\n",
    "    split_index = int(train_proportion * data.shape[0])\n",
    "    training_idx = indices[:split_index]\n",
    "    test_idx = indices[split_index:]\n",
    "\n",
    "    return data[training_idx, :], data[test_idx, :]\n",
    "\n",
    "\n",
    "def cross_val(data, k=10):\n",
    "    assert k >= 2\n",
    "    datasets = []\n",
    "\n",
    "    if data.shape[0] % k != 0:\n",
    "        print(\"warning: this dataset contains {} entries and cannot be equally divided into {} chunks for cross-validation.\".format(data.shape[0], k))\n",
    "        print(\"Prutruding rows will be dropped.\")\n",
    "        data = data[ : (data.shape[0] // k) * k]\n",
    "\n",
    "    for i in range(k):\n",
    "        data_chunks = np.split(data, k)\n",
    "\n",
    "        val_data = data_chunks.pop(i)\n",
    "        train_data = np.concatenate(data_chunks)\n",
    "        datasets.append((train_data, val_data))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def get_labels_and_features(dataset:np.ndarray)->Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return labels and features from a given dataset.\n",
    "    :return: labels, features\n",
    "    \"\"\"\n",
    "    # [0] age - [10] active\n",
    "    # [11] cardio\n",
    "    # [12] scaled age - [22]...\n",
    "    col = {\n",
    "        'age':0, 'gender': 1, 'height':2 , 'weight': 3, 'ap_hi': 4, 'ap_lo': 5, 'cholesterol': 6,\n",
    "        'gluc': 7, 'smoke': 8, 'alco': 9, 'active': 10, 'cardio': 11, 'age_standardized': 12,\n",
    "        'height_standardized': 13, 'weight_standardized': 14, 'ap_hi_fixed':15, 'ap_lo_fixed': 16\n",
    "           }\n",
    "    feature_list = [\n",
    "        col['age_standardized'], col['gender'], col['height_standardized'], col['weight_standardized'],\n",
    "        col['cholesterol'], col['gluc'], col['active'], col['ap_lo_fixed'],col['ap_hi_fixed']\n",
    "        ]\n",
    "    labels = dataset[:, col['cardio']]\n",
    "    features = dataset[:, feature_list]\n",
    "    return labels, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_data = df.to_numpy()\n",
    "train_data, test_data = train_test_split(cardio_data, shuffle=False)\n",
    "datasets = cross_val(train_data, k=2)\n",
    "\n",
    "# only use a single dataset for now\n",
    "#train_set, val_set = datasets[0]\n",
    "#y, X = get_labels_and_features(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate minimized hypothesis function $h$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average model accuracy for sigma=0.5, lambda=0.1\n",
      "train: 0.8671428571428572\n",
      "val: 0.6171428571428571\n",
      "\n",
      "Average model accuracy for sigma=0.5, lambda=1.0\n",
      "train: 0.8671428571428572\n",
      "val: 0.6142857142857143\n",
      "\n",
      "Average model accuracy for sigma=1.0, lambda=0.1\n",
      "train: 0.7\n",
      "val: 0.6185714285714285\n",
      "\n",
      "Average model accuracy for sigma=1.0, lambda=1.0\n",
      "train: 0.7057142857142857\n",
      "val: 0.6157142857142857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def score(h, X, y):\n",
    "    predictions = h(X)\n",
    "    #print(predictions)\n",
    "\n",
    "    score = (predictions*y >= 0).astype(int)\n",
    "    return score.sum()/score.shape[0]\n",
    "\n",
    "def train_n_score(datasets, sigma, lam):\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "    for train_set, val_set in datasets:\n",
    "        y_train, X_train = get_labels_and_features(train_set)\n",
    "        y_val, X_val = get_labels_and_features(val_set)\n",
    "\n",
    "        h = kernel_lr(X_train, y_train, sigma=sigma, lam=lam)\n",
    "\n",
    "        train_accuracy.append(score(h, X_train, y_train))\n",
    "        val_accuracy.append(score(h, X_val, y_val))\n",
    "\n",
    "    print(f'Average model accuracy for sigma={sigma}, lambda={lam}')\n",
    "    print(f'train: {sum(train_accuracy)/len(train_accuracy)}')\n",
    "    print(f'val: {sum(val_accuracy)/len(val_accuracy)}\\n')\n",
    "\n",
    "\n",
    "sigmas=[0.5, 1.]\n",
    "lambdas=[0.1, 1.]\n",
    "\n",
    "for sigma in sigmas:\n",
    "    for lam in lambdas:\n",
    "        train_n_score(datasets, sigma, lam)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO Leo: show generalization error in plot\n",
    "\n",
    "TODO: measures against over and underfitting\n",
    "\n",
    "TODO Jan: interpretation of result -> plot von Jan, bzw. für relevante Variablen\n",
    "\n",
    "TODO: Ausblick -> z.B. andere Lern algorithm, ensemble Methoden wie adaboost, ...\n",
    "Ausblick wenn mehr Zeit - do we want to show any differences compared to decision trees , SVM's, etc...\n",
    "\n",
    "TODO Vincent: Metrics -> Accuracy, F1 score\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further Cases\n",
    "\n",
    "Cardiovascular disease Prediction\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}