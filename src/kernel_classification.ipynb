{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cardiovascular disease Prediction\n",
    "\n",
    "Cardiovascular diseases are the No.1 reason for all deaths in the world [[WHO](https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds) )] and many are preventable.\n",
    "\n",
    "There are many subgroups of different kind of diseases centred around the heart and the blood vessels:\n",
    "\n",
    "- coronary heart disease – disease of the blood vessels supplying the heart muscle\n",
    "- cerebrovascular disease – disease of the blood vessels supplying the brain\n",
    "- peripheral arterial disease – disease of blood vessels supplying the arms and legs\n",
    "- rheumatic heart disease – damage to the heart muscle and heart valves from rheumatic fever, caused by streptococcal bacteria\n",
    "- congenital heart disease – malformations of heart structure existing at birth\n",
    "- deep vein thrombosis and pulmonary embolism – blood clots in the leg veins, which can dislodge and move to the heart and lungs.\n",
    "\n",
    "The typical risk factors of Cardiovascular diseases are:\n",
    "\n",
    "- Behavioral:\n",
    "    - diet,\n",
    "    - physical inactivity,\n",
    "    - tobacco use and\n",
    "    - harmful use of alcohol.\n",
    "\n",
    "- Medical Values:\n",
    "    - raised blood pressure,\n",
    "    - raised blood glucose,\n",
    "    - raised blood lipids and\n",
    "    - overweight & obesity\n",
    "\n",
    "\n",
    "The risk factors mentioned above are quite numerous, and a doctor is only so good at asking questions and data control.\n",
    "Our motivation for this project is to help doctors and patients by providing a tool that is able to make a prediction if one it subjected to a cardiovascular disease.\n",
    "This prediction is based on objective, measured and subjective information where most details can be easily obtained by the patients themselves or simple measurements.\n",
    "The Machine Learning model might therefore be a useful tool to bring attention to early stages and to minimize examination mistakes by providing a second opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "To make this prediction possible we found a dataset  with containing said risk factors as features, and the label based on the Cardiovascular disease status.\n",
    "\n",
    "This dataset is taken from Kaggle.com and can be found [here](https://www.kaggle.com/sulianova/cardiovascular-disease-dataset).\n",
    "\n",
    "The dataset has data recordings of 70 000 patients including 11 different Features and 1 label. There are 3 types of input features:\n",
    "\n",
    "* Objective:    factual information\n",
    "* Examination:  results of medical examination\n",
    "* Subjective:   information given by the patient\n",
    "\n",
    "| [index] id| [0] age| [1] gender| [2] height| [3] weight|\n",
    "| ---| ---| ---| ---| ---|\n",
    "| int| int| 1 or 2 | int| float|\n",
    "| -| days| categorical code (2=men)| cm| kg |\n",
    "| -| Objective| Objective| Objective| Objective |\n",
    "\n",
    "| [4] ap_hi| [5] ap_lo| [6] cholesterol| [7] gluc|\n",
    "| ---| ---| ---| ---|\n",
    "| int| int| 1, 2, 3 | 1, 2, 3 |\n",
    "| -| -| normal, above normal, well above normal| normal, above normal, well above normal|\n",
    "| Examination| Examination| Examination| Examination|\n",
    "\n",
    ">Note: ap_hi = Systolic blood pressure, ap_lo = Diastolic blood pressure, gluc = Glucose\n",
    "\n",
    "| [8] smoke| [9] alco| [10] active| [11] cardio|\n",
    "| ---| ---| ---| ---|\n",
    "| binary| binary| binary| binary |\n",
    "| -| -| -| categorical code|\n",
    "| Subjective| Subjective| Subjective| Target|\n",
    "\n",
    ">Note: alco = Alcohol intake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - Data Correlation\n",
    "\n",
    "A good start for choosing the correct prediction algorithm is an exploratory data analysis (EDA).\n",
    "This analysis looks at the data and shows potential correlations or problems.\n",
    "\n",
    "More information and details can be found in this [report](../docs/CardioVascular-Diseas-Prediction.html).\n",
    "This report was compiled using the [pandas-profiling tool](https://github.com/pandas-profiling/pandas-profiling).\n",
    "\n",
    " - Our results show that some data is corrupt and needs to be fixed (e.g. implausible blood pressure values).\n",
    " - The data is not linear separable, so we definitely need a good feature function or kernel.\n",
    " - The result of the prediction should be a label and not a value, so we want a classifier.\n",
    "\n",
    "\n",
    "Based on the these results a good choice for a prediction algorithm in our project is the **Kernel Logistic Regression**.\n",
    "\n",
    "\n",
    "Based on the WHO's [cardiovascular risk charts](https://www.who.int/news/item/02-09-2019-who-updates-cardiovascular-risk-charts)\n",
    "being male/elderly/a smoker or having diabetes/high cholesterol levels are the most prominent risk factors for having a cardiovascular disease.\n",
    "\n",
    "\n",
    "To get a better understanding of our data set and the relation between the variables, we compute the correlation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "plot_df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')#.sample(1000, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(plot_df.corr(), vmin=-1, vmax=1, annot=True, cmap='vlag')\n",
    "plt.title('Correlation Matrix', fontdict={'fontsize':14}, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Abnormalities\n",
    "\n",
    "By looking into the last column, we can see the variables of our dataset which correlate most to a cardiovascular disease.\n",
    "In our case the biggest influences are the features `age`, `cholesterol`, `weight`, `gulcose` and blood pressure (`ap_lo`/`ap_hi`) (with decreasing significance).\n",
    "\n",
    "This aligns mostly with the medical values mentioned by the WHO.\n",
    "\n",
    "Contrary to the aforementioned WHO's cardiovascular risk charts, the `gender` of a person is of minor importance.\n",
    "\n",
    "Surprisingly, `smoking` and high `alcohol` intake seem to lessen the risk of cardiovascular disease.\n",
    "Perhaps these features may have gotten mixed up during data collection, but as this is a kaggle-dataset with no reference to the original source,\n",
    "we cannot know for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the features `age` and `weight` in order to visualize the trend in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# take age & round days to years\n",
    "age = plot_df['age']\n",
    "age_divider = 1.0/365.0\n",
    "age = age * age_divider\n",
    "#\n",
    "# create age data in correlation with cardio\n",
    "age_data = pd.concat([age, plot_df['weight'], plot_df['cardio']], axis=1, join='inner')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='age', y='weight', data=age_data, hue='cardio', palette=\"seismic\")\n",
    "# TODO label into ['cardio true', 'cardio false']\n",
    "plt.xlabel('age in years')\n",
    "plt.ylabel('weight in kg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there tend to be more healthy cases in the lower left part of the scatter plot (low age/ low weight) than in the upper right (high age/ high weight).\n",
    "Nevertheless, it should also be noted that there are many outliers, as well as some values that don't make any sense (e.g. an adult person with only 10 kg).\n",
    "\n",
    "It may be advantageous to exclude these outliers before training our model."
   ]
  },
  {
   "source": [
    "The plot also shows that our dataset seems to have a weird peculiarity in the feature `age`.\n",
    "Instead of a smooth distribution over the years, some clusters can be observed.\n",
    "While we do convert the age (given in days) into years, we keep our value as a float and do not round the value.\n",
    "We assume that some kind of limitation in the creation process is the reason for these jumps in the data. Our dataset might just be a subset of an even larger dataset as it mostly includes patients in the age group 39-66 years. \n",
    "This peculiarity does cause some concern for trust into data, but for our project it is important that no unexpected trends emerge, and our dataset is still a representation of the real world.\n",
    "\n",
    "Luckily this is the case: when compared with the WHO's risk tables the trend of \"higher age, more of cases of cardio diseases\" is still valid."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Duplicate Entries\n",
    "\n",
    "Our dataset is clearly not very clean.\n",
    "If it also contains duplicate rows, they may end up in both the training set and the test set, which might be a problem\n",
    "for estimating the generalization error if they are the result of the poor data collection process.\n",
    "However, with a dataset consisting of 70000 entries some duplicates are to be expected.\n",
    "\n",
    "There appears to be only 41 entries with the exact same feature values - with only 24 of these also matching the same label.\n",
    "We conclude that there is no need for deleting these duplicate rows, because they probably represent real world data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = plot_df.columns[:-1]\n",
    "\n",
    "duplicates_features = plot_df[plot_df.duplicated(features)]\n",
    "duplicates = plot_df[plot_df.duplicated()]\n",
    "\n",
    "print(f\"Our dataset contains {len(duplicates_features)} duplicates (varying labels)\")\n",
    "print(f\"Our dataset contains {len(duplicates)} duplicates (matching labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean up Jupyter Notebook for better performance\n",
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Kernel Logistic Regression\n",
    "\n",
    "#### Formulary\n",
    "Features: $x$\n",
    "\n",
    "Labels: $y$\n",
    "\n",
    "Hypothesis Function: $h(x)$\n",
    "\n",
    "Loss Function: $l(h(x), y)$\n",
    "\n",
    "Regularization Term: $\\Omega(w)$\n",
    "\n",
    "Objective Function: $ J(w) = \\frac{1}{m} \\sum_{i=1}^{m} l(h(x_i), y_i) + \\Omega(w)$\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing corrupted entries\n",
    "\n",
    "As we saw in the explorative data analysis, our dataset includes corrupted values (e.g. blood pressures above 20000) that need to be expelled before we can make a solid prediction. \n",
    "\n",
    "Therefore we define reasonable value ranges for the systolic/diastolic blood pressures, weight and height features in order to account for any errors during the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_count = len(df)\n",
    "\n",
    "df = df[(50 <= df['ap_lo']) & (df['ap_lo'] <= 150)]\n",
    "df = df[(100 <= df['ap_hi']) & (df['ap_hi'] <= 200)]\n",
    "\n",
    "df = df[(25 <= df['weight']) & (df['weight'] <= 400)]\n",
    "df = df[(100 <= df['height']) & (df['height'] <= 210)]\n",
    "\n",
    "new_count = org_count - len(df)\n",
    "print(f\"{new_count} entries have been excluded due to implausible feature values.\")\n",
    "\n",
    "del new_count, org_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "The information provided by the dataset depends on the category. Some are binary values like `gender`, others are categorical like `cholesterol` or numerical like `weight` and `height`.\n",
    "We need to normalize these data points to a similar scale. This process is called feature scaling.\n",
    "\n",
    "Feature Scaling is necessary because if the range of raw data varies widely, it can be the case that the objective function of some machine learning algorithms will not work properly.\n",
    "This is also the case for the Kernel Feature Function.\n",
    "The main reason for this is that the Kernel Logistic Regression Algorithm calculates the Squared/Euclidean distance between the Feature Points.\n",
    "If one Feature has a broad value range the distance is determined and influenced mostly by this particular feature.\n",
    "\n",
    "Since the Cardiovascular Disease Dataset has a broad value range for example for the Feature `age` (given in days), we standardize our data such that all features have a mean of zero and a standard deviation of 1.\n",
    "We use the standardization formula:\n",
    "\n",
    "$\\tilde{x_i} = \\frac{x_i - \\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def standardize(feature):\n",
    "    return (feature-statistics.mean(feature)) / statistics.stdev(feature)\n",
    "\n",
    "df_standardized = pd.DataFrame({})\n",
    "\n",
    "# numerical values\n",
    "df_standardized['age'] = standardize(df['age'])\n",
    "df_standardized['height'] = standardize(df['height'])\n",
    "df_standardized['weight'] = standardize(df['weight'])\n",
    "df_standardized['ap_hi'] = standardize(df['ap_hi'])\n",
    "df_standardized['ap_lo'] = standardize(df['ap_lo'])\n",
    "\n",
    "# binary/categorical values\n",
    "df_standardized['gender'] = df['gender'].apply(lambda t: -1 if t==1 else 1).values\n",
    "df_standardized['smoke'] = df['smoke'].apply(lambda t: -1 if t==0 else 1).values\n",
    "df_standardized['alco'] = df['alco'].apply(lambda t: -1 if t==0 else 1).values\n",
    "df_standardized['active'] = df['active'].apply(lambda t: -1 if t==1 else 1).values\n",
    "df_standardized['cholesterol'] = df['cholesterol'].apply(lambda t: -1 if t==1 else (0 if t==2 else 1)).values\n",
    "df_standardized['gluc'] = df['gluc'].apply(lambda t: -1 if t==1 else (0 if t==2 else 1)).values\n",
    "\n",
    "df_standardized['cardio'] = df['cardio'].apply(lambda t: 1 if t==1 else -1).values\n",
    "\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kernel / Feature function\n",
    "For the best result we want to include many of our important risk factors into our feature function.\n",
    "\n",
    "Based on our EDA we can say that our data is - as it is - not linear separable. There might be a case where the data is linear separable if aligned properly, but instead of finding the best feature function by introducing extra dimensions, we kernelize our feature function.\n",
    "\n",
    "$h(x) = w^T * \\Phi(x_i)= w^T*K(x,z) = w^T * (x^T*z+1)^d $"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing the objective function + helper functions\n",
    "\n",
    "#### Squared Exponential Kernel $k(x,z)$\n",
    "$k(x,z) = exp(− x^Tx−2x^Tz+z^Tz/ 2σ^2) = exp(sqdist(x,z)/2σ^2)$\n",
    "\n",
    "#### Hypothesis Function $h(x)$\n",
    "$h_\\alpha(x) = \\alpha K = \\sum_{j=1}^{m} \\alpha_j k(x_j,x)$\n",
    "\n",
    "#### Loss Function $l(h(x),y)$\n",
    "\n",
    "logistic loss:\n",
    "\n",
    "$l_{logistic}(h_\\alpha(x), y) = log(1 + e^{−y·h(x)})= log(1 + exp(−y · h_\\alpha(x)))$\n",
    "\n",
    "#### Regularization Term: $\\Omega(w)$\n",
    "\n",
    "$\\Omega(\\alpha) = \\lambda l_2 = \\lambda\\alpha^{\\intercal}K\\alpha$\n",
    "\n",
    "\n",
    "#### Objective Function J\n",
    "\n",
    "kernelized logistic regression:\n",
    "\n",
    "$\n",
    "J(\\alpha)\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} l(h(x_i), y_i) + \\Omega(\\alpha)\n",
    "= \\frac{1}{m} \\sum_{i=1}^m  \\log \\big(1 + \\exp\\big(-y_i \\cdot \\sum_{j=1}^{m} \\alpha_j k(x_j,x_i)\\big) \\big) + \\lambda \\alpha^{\\intercal}K\\alpha\n",
    "$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sqdist(X, Z):\n",
    "    p1 = np.sum(X**2, axis=1)[:, np.newaxis]\n",
    "    p2 = np.sum(Z**2, axis=1)\n",
    "    p3 = -2 * np.dot(X, Z.T)\n",
    "    return p1+p2+p3\n",
    "\n",
    "def sq_exp(X, Z, sigma):\n",
    "    return np.exp(-sqdist(X, Z)/(2*sigma**2) )\n",
    "\n",
    "\n",
    "def J(α, X, y, sigma, lam):\n",
    "    K = sq_exp(X, X, sigma)\n",
    "    m = X.shape[0]\n",
    "    total_loss = 0\n",
    "    regularization = lam * np.dot(np.dot(np.transpose(α), K), α)\n",
    "\n",
    "    for i in range(m):\n",
    "        prediction = α @ K[i]\n",
    "        logistic_loss = np.log(1 + np.exp(-y[i] * prediction))\n",
    "        total_loss += logistic_loss\n",
    "\n",
    "    mean_loss = total_loss / m  + regularization\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the gradient of the regularized kernlized logistic regression objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dJ(α, X, y, sigma, lam):\n",
    "    K = sq_exp(X, X, sigma)\n",
    "    m = X.shape[0]\n",
    "    gradient = 0\n",
    "    regularization = 2*lam * np.dot( K, α)\n",
    "\n",
    "    for i in range(m):\n",
    "        prediction = α @ K[i]\n",
    "\n",
    "        numerator = -y[i] * K[i]\n",
    "        denominator = 1 + np.exp(y[i] * prediction)\n",
    "        gradient += numerator / denominator\n",
    "\n",
    "    mean_gradient = gradient / m + regularization\n",
    "    return mean_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def kernel_lr(X, y, sigma, lam):\n",
    "    # implementation of kernel ridge regression using the scipy optimizer gradient descent\n",
    "    α = np.zeros(X.shape[0],)\n",
    "    α = minimize(J, α, args=(X, y, sigma, lam), jac=dJ, method='CG').x\n",
    "    h = lambda Z: np.dot(α, sq_exp(X, Z, sigma))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in Train / Validation / Test\n",
    "\n",
    "In order to determine the quality of our model, we have to estimate its generalization error (on new data).\n",
    "Therefore, it is common to split the dataset into a training set and a test set which will be exclusively used for training and testing respectively. \n",
    "\n",
    "However, the Squared Exponention Kernel depends on the parameter $\\sigma$ and the-$\\lambda$ parameter determines how much we regularize our model. First, we need to optimize these parameters by training multiple models with varying ($\\sigma$, $\\lambda$)-pairs. The resulting models need to be compared, but we cannot use our test set for the gerneralization error, because we would be \"cherry-picking\" the model that best works for the test set instead of real world data.\n",
    "\n",
    "In order to counteract this problem, we use cross-validation for estimating the gerneralization error. Our training set needs to be split into $k$ folds. We than train $k$ models on $k-1$ training folds und use the last fold for validating our models (each model with a different validation fold). The average over all validation errors serves as a guide for choosing the best parameters $\\sigma$ and $\\lambda$. \n",
    "\n",
    "We will train one last model with optimal parameters on the entire training set, and only than use the test set for estimating the gerneralization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_sample_count, shuffle=True):\n",
    "    mask = np.full(data.shape[0], False)\n",
    "    mask[:train_sample_count] = True\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(mask)\n",
    "\n",
    "    train_data = data[mask]\n",
    "    test_data = data[~mask]\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(data, k=10):\n",
    "    assert k >= 2\n",
    "    datasets = []\n",
    "\n",
    "    if data.shape[0] % k != 0:\n",
    "        print(\"warning: this dataset contains {} entries and cannot be equally divided into {} chunks for cross-validation.\".format(data.shape[0], k))\n",
    "        print(\"Prutruding rows will be dropped.\")\n",
    "        data = data[ : (data.shape[0] // k) * k]\n",
    "\n",
    "    for i in range(k):\n",
    "        data_chunks = np.split(data, k)\n",
    "\n",
    "        val_data = data_chunks.pop(i)\n",
    "        train_data = np.concatenate(data_chunks)\n",
    "        datasets.append((train_data, val_data))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "source": [
    "It is common to use 70% of the data for training purposes and 30% for testing. However, training the Kernel Logistic Regression Algorithm for lots of datapoints is computationally expensive. Unfortunately we already reach our computational limit (in a reasonable amount of time) with ~1000 datapoints. We have an obundantly large dataset and predicting on an already trained model is quite fast. Therefore, our test set will be unusually large compared to our training set.\n",
    "\n",
    "The computational limitations also force us to use a small number of folds for cross-validation. We will only use 2-fold-cross-validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_data = df_standardized.to_numpy()\n",
    "\n",
    "training_samples = 500\n",
    "fold_count = 2\n",
    "\n",
    "train_data, test_data = train_test_split(cardio_data, training_samples, shuffle=True)\n",
    "cross_val_datasets = cross_val(train_data, k=fold_count)\n",
    "\n",
    "print(f\"training set size: {train_data.shape[0]} \")\n",
    "print(f\"test set size: {test_data.shape[0]} \")\n",
    "\n",
    "del cardio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose Features\n",
    "\n",
    "Instead of using all features available we choose to only take these features that align with our theoretical background.\n",
    "This means we look at the correlation matrix from the EDA, which results in following claims:\n",
    "\n",
    "#### Good Features:\n",
    "`age`, `weight`, `cholesterol`, `gluc`, `ap_lo` and `ap_hi` support our hypothesis.\n",
    "\n",
    "`active` ?????\n",
    "TODO: what about being active thats a feature that should be useful?\n",
    "\n",
    "As these features a helpful for our model we include these as our features.\n",
    "\n",
    "#### Neutral Features:\n",
    "`gender` doesn't show high enough correlation although it would support our hypothesis.\n",
    "\n",
    "`height` should have no influence on having a cardiovascular disease.\n",
    "\n",
    "For faster results while computing the prediction we leave these features out.\n",
    "\n",
    "#### Bad Features:\n",
    "`alco` and `smoking` normally show a strong correlation towards having a cardiovascular disease.\n",
    "A minor opposite correlation can be found in out dataset.\n",
    "\n",
    "For a better correlation we leave these features out."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_labels_and_features(dataset:np.ndarray)->Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return labels and features from a given dataset.\n",
    "    :return: labels, features\n",
    "    \"\"\"\n",
    "    col = {\n",
    "        'age': 0, 'height': 1 , 'weight': 2, 'ap_hi': 3, 'ap_lo': 4, 'gender': 5, 'smoke': 6, \n",
    "        'alco': 7, 'active': 8, 'cholesterol': 9, 'gluc': 10, 'cardio': 11\n",
    "           }\n",
    "    feature_list = [\n",
    "        col['age'], col['weight'], col['cholesterol'], col['gluc'], col['ap_lo'], col['ap_hi'], col['active'], col['gender']\n",
    "        ]\n",
    "\n",
    "    labels = dataset[:, col['cardio']]\n",
    "    features = dataset[:, feature_list]\n",
    "    return labels, features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Parameter Optimization\n",
    "\n",
    "TODO Vinc: Beschreiben unserer measures against over and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def score(h, X, y):\n",
    "    #TODO replace score with precision/recall score from metrics\n",
    "    predictions = h(X)\n",
    "\n",
    "    score = (predictions*y >= 0).astype(int)\n",
    "    return score.sum()/score.shape[0]\n",
    "\n",
    "\n",
    "sigmas=[0.5, 1., 2., 4., 8.]\n",
    "lambdas=[1.]\n",
    "\n",
    "for lam in lambdas:\n",
    "    for sigma in sigmas:\n",
    "        train_scores = []\n",
    "        val_scores = []\n",
    "        for train_set, val_set in cross_val_datasets:\n",
    "            y_train, X_train = get_labels_and_features(train_set)\n",
    "            y_val, X_val = get_labels_and_features(val_set)\n",
    "\n",
    "            h = kernel_lr(X_train, y_train, sigma=sigma, lam=lam)\n",
    "\n",
    "            train_scores.append(score(h, X_train, y_train))\n",
    "            val_scores.append(score(h, X_val, y_val))\n",
    "        \n",
    "        print(f'Average model accuracy for sigma={sigma}, lambda={lam}')\n",
    "        print(f'train: {sum(train_scores)/len(train_scores)}')\n",
    "        print(f'val: {sum(val_scores)/len(val_scores)}\\n')\n",
    "\n",
    "\n",
    "# TODO Jan: show generalization error in plot, or plots for lambda / simga changes\n",
    "# compare: https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics\n",
    "\n",
    "For a final evaluation of our model we calculate use common metrics for comparability.\n",
    "\n",
    "Our metrics are based around the confusion matrix.\n",
    "\n",
    "True positive = $TP$\n",
    "\n",
    "True negative = $TN$\n",
    "\n",
    "False positive = $FP$\n",
    "\n",
    "False negative = $FN$\n",
    "\n",
    "- Accuracy: the proportion of correct predictions among the total number of predictions made.\n",
    "\n",
    "    $Accuracy = (TP + TN) / (TP + TN + FP + FN) $\n",
    "\n",
    "    So how accurate is our prediction of datapoints. Note: an overfitted model has a high accuracy.\n",
    "\n",
    "- Precision: the proportion of the true predictions among all the positive predictions made.\n",
    "\n",
    "    $Precision = TP / (TP + FP)$\n",
    "\n",
    "    So how serious do we have to take a positive result. Note: when this score is low, we have lots of false alarms.\n",
    "\n",
    "- Recall: the proportion of the true positive predictions among the total amount of relevant samples in the dataset.\n",
    "\n",
    "    $Recall = TP / (TP + FN)$\n",
    "\n",
    "    So how good is our model in detecting the disease. Note: when this score is low, we missed lots of cases where patients have the disease.\n",
    "\n",
    "- F1: the harmonic mean of the precision and recall.\n",
    "\n",
    "    $F1 = 2 * (precision * recall) / (precision + recall)$\n",
    "\n",
    "    As neither accuracy, precision and recall can judge the model's performance on their own we use the F1 score as a final comparsion value.\n",
    "    This works because the harmonic mean puts the focus on the small values.\n",
    "    So when either precision **or** recall is performing badly it reflects in the F1 score.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train hypothesis function on entire training set (including validation set) on best parameters\n",
    "sigma = 0.9\n",
    "lam = 1e-4\n",
    "y_train, X_train = get_labels_and_features(train_data)\n",
    "y_test, X_test = get_labels_and_features(test_data)\n",
    "h = kernel_lr(X_train, y_train, sigma=sigma, lam=lam)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "X, y =  X_test, y_test\n",
    "\n",
    "predictions = h(X)\n",
    "matching_score = (predictions * y >= 0).astype(int)\n",
    "\n",
    "# true disease, false alarm, no disease, missed disease\n",
    "tp, fp, tn, fn = [], [], [], []\n",
    "for index, score in enumerate(predictions):\n",
    "    label = y[index]\n",
    "    if score > 0 and label > 0:\n",
    "        tp.append(score)\n",
    "    elif score > 0 and label < 0:\n",
    "        fp.append(score)\n",
    "    elif score < 0 and label < 0:\n",
    "        tn.append(score)\n",
    "    elif score < 0 and label > 0:\n",
    "        fn.append(score)\n",
    "\n",
    "\n",
    "accuracy = matching_score.sum() / matching_score.shape[0]\n",
    "precision = len(tp)/ (len(tp)+ len(fp))\n",
    "recall = len(tp) / (len(tp) + len(fn))\n",
    "f_1 = 2 * ( (precision * recall) / (precision + recall) )\n",
    "\n",
    "\n",
    "print('\\t---- Metrics ----\\n'\n",
    "    f'Accuracy:\\t {accuracy}\\n'\n",
    "    f'Precision:\\t {precision}\\n'\n",
    "    f'Recall:\\t\\t {recall}\\n'\n",
    "    f'F1:\\t\\t {f_1}\\n')\n",
    "\n",
    "\n",
    "print('\\t---- Confusion Matrix ----')\n",
    "plot_data = [[len(tn), len(fp)],\n",
    "             [len(fn), len(tp)]]\n",
    "sns.heatmap(plot_data, annot=True)\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('actual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f'We predicted correctly that {len(tn)} patients have no disease.\\n'\n",
    "    f'We predicted correctly that {len(tp)} patients have a cardio disease.\\n'\n",
    "    f'But we missed {len(fn)} patients and gave {len(fp)} false alarms.\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ToDo: Generalisierungsfehler schätzen mit Test-Datensatz\n",
    "\n",
    "true positive rate (tpr) & true negative rate (tnr) ?\n",
    "tpr = recall\n",
    "tnr =  len(tn)/(len(tn)+len(fp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Further Cases\n",
    "\n",
    "Cardiovascular disease Prediction\n",
    "\n",
    "TODO Vinc: Ausblick -> z.B. andere Lern algorithm, ensemble Methoden wie adaboost, ...\n",
    "Ausblick wenn mehr Zeit - do we want to show any differences compared to decision trees , SVM's, etc...\n",
    "\n",
    "TODO: compare our results with others on kaggle\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}