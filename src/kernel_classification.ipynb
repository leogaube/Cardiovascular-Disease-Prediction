{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cardiovascular disease Prediction\n",
    "\n",
    "Cardiovascular diseases are the No.1 reason for all deaths in the world [[WHO](https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds) )] and many are preventable.\n",
    "\n",
    "There are many subgroups of different kind of diseases centred around the heart and the blood vessels:\n",
    "\n",
    "- coronary heart disease – disease of the blood vessels supplying the heart muscle\n",
    "- cerebrovascular disease – disease of the blood vessels supplying the brain\n",
    "- peripheral arterial disease – disease of blood vessels supplying the arms and legs\n",
    "- rheumatic heart disease – damage to the heart muscle and heart valves from rheumatic fever, caused by streptococcal bacteria\n",
    "- congenital heart disease – malformations of heart structure existing at birth\n",
    "- deep vein thrombosis and pulmonary embolism – blood clots in the leg veins, which can dislodge and move to the heart and lungs.\n",
    "\n",
    "The typical risk factors of Cardiovascular diseases are:\n",
    "\n",
    "- Behavioral:\n",
    "    - diet,\n",
    "    - physical inactivity,\n",
    "    - tobacco use and\n",
    "    - harmful use of alcohol.\n",
    "\n",
    "- Medical Values:\n",
    "    - raised blood pressure,\n",
    "    - raised blood glucose,\n",
    "    - raised blood lipids and\n",
    "    - overweight & obesity\n",
    "\n",
    "\n",
    "The risk factors mentioned above are quite numerous, and a doctor is only so good at asking questions and data control.\n",
    "Our motivation for this project is to help doctors and patients by providing a tool that is able to make a prediction if one it subjected to a cardiovascular disease.\n",
    "This prediction is based on objective, measured and subjective information where most details can be easily obtained by the patients themselves or simple measurements.\n",
    "The Machine Learning model might therefore be a useful tool to bring attention to early stages and to minimize examination mistakes by providing a second opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "To make this prediction possible we found a dataset with said risk factors as features, and the label based on the Cardiovascular disease status.\n",
    "\n",
    "This dataset is taken from Kaggle.com and can be found [here](https://www.kaggle.com/sulianova/cardiovascular-disease-dataset).\n",
    "\n",
    "The dataset has data recordings of 70 000 patients containing 11 different Features and one label. There are three types of input features:\n",
    "\n",
    "* Objective:    factual information\n",
    "* Examination:  results of medical examination\n",
    "* Subjective:   information given by the patient\n",
    "\n",
    "| [index] id| [0] age| [1] gender| [2] height| [3] weight|\n",
    "| ---| ---| ---| ---| ---|\n",
    "| int| int| 1 or 2 | int| float|\n",
    "| -| days| categorical code (2=men)| cm| kg |\n",
    "| -| Objective| Objective| Objective| Objective |\n",
    "\n",
    "| [4] ap_hi| [5] ap_lo| [6] cholesterol| [7] gluc|\n",
    "| ---| ---| ---| ---|\n",
    "| int| int| 1, 2, 3 | 1, 2, 3 |\n",
    "| -| -| normal, above normal, well above normal| normal, above normal, well above normal|\n",
    "| Examination| Examination| Examination| Examination|\n",
    "\n",
    ">Note: ap_hi = Systolic blood pressure, ap_lo = Diastolic blood pressure, gluc = Glucose\n",
    "\n",
    "| [8] smoke| [9] alco| [10] active| [11] cardio|\n",
    "| ---| ---| ---| ---|\n",
    "| binary| binary| binary| binary |\n",
    "| -| -| -| categorical code|\n",
    "| Subjective| Subjective| Subjective| Target|\n",
    "\n",
    ">Note: alco = Alcohol intake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - Data Correlation\n",
    "\n",
    "A good start for choosing the correct prediction algorithm is an exploratory data analysis (EDA).\n",
    "This analysis looks at the data and shows potential correlations or problems.\n",
    "\n",
    "More information and details can be found in this [report](../docs/CardioVascular-Diseas-Prediction.html).\n",
    "This report was compiled using the [pandas-profiling tool](https://github.com/pandas-profiling/pandas-profiling).\n",
    "\n",
    " - The report shows that some data is corrupt and needs to be fixed (e.g. implausible blood pressure values).\n",
    " - The data is not linear separable, so we definitely need a good feature function or kernel.\n",
    " - The result of the prediction should be a category and not a continuous quantity, so we want a classifier.\n",
    "\n",
    "\n",
    "Based on the these results a good choice for a prediction algorithm in our project is the **Kernel Logistic Regression**.\n",
    "\n",
    "\n",
    "Based on the WHO's [cardiovascular risk charts](https://www.who.int/news/item/02-09-2019-who-updates-cardiovascular-risk-charts)\n",
    "being male/elderly/a smoker or having diabetes/high cholesterol levels are the most prominent risk factors for having a cardiovascular disease.\n",
    "\n",
    "\n",
    "To get a better understanding of our dataset and the relation between the variables, we compute the correlation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "plot_df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')#.sample(1000, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(plot_df.corr(), vmin=-1, vmax=1, annot=True, cmap='vlag')\n",
    "plt.title('Correlation Matrix', fontdict={'fontsize':14}, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Abnormalities\n",
    "\n",
    "By looking into the last column, we can see the variables of our dataset which correlate most to a cardiovascular disease.\n",
    "In our case the biggest influences are, with decreasing significance, the features `age`, `cholesterol`, `weight`, `gulcose` and blood pressure (`ap_lo`/`ap_hi`).\n",
    "\n",
    "This aligns mostly with the medical values mentioned by the WHO.\n",
    "\n",
    "Contrary to the aforementioned WHO's cardiovascular risk charts, the `gender` of a person is of minor importance.\n",
    "\n",
    "Surprisingly, `smoking` and high `alcohol` intake seem to lessen the risk of cardiovascular disease.\n",
    "Perhaps these features may have gotten mixed up during data collection, but as this is a kaggle-dataset with no reference to the original source,\n",
    "we cannot know for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the features `age` and `weight` in order to visualize the trend in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# take age & round days to years\n",
    "age = plot_df['age']\n",
    "age_divider = 1.0/365.0\n",
    "age = age * age_divider\n",
    "\n",
    "# create age data in correlation with cardio\n",
    "age_data = pd.concat([age, plot_df['weight'], plot_df['cardio']], axis=1, join='inner')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='age', y='weight', data=age_data, hue='cardio', palette=\"seismic\")\n",
    "plt.xlabel('age in years')\n",
    "plt.ylabel('weight in kg')\n",
    "\n",
    "# TODO Leo/Jan: label into ['cardio true', 'cardio false']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there tend to be more healthy cases in the lower left part of the scatter plot (low age/ low weight) than in the upper right (high age/ high weight).\n",
    "Nevertheless, it should also be noted that there are many outliers, as well as some values that don't make any sense (e.g. an adult person with only 10 kg).\n",
    "\n",
    "It may be advantageous to exclude these outliers before training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot also shows that our dataset seems to have a weird peculiarity in the feature `age`.\n",
    "Instead of a smooth distribution over the years, some clusters can be observed.\n",
    "While we do convert the age (given in days) into years, we keep our value as a float and do not round the value.\n",
    "We assume that some kind of limitation in the creation process is the reason for these jumps in the data.\n",
    "Our dataset might just be a subset of an even larger dataset as it mostly includes patients in the age group 39-66 years.\n",
    "This peculiarity does cause some concern for trust into the data source, but for our project it is important that no unexpected trends emerge, and our dataset is still a representation of the real world.\n",
    "\n",
    "Luckily this is the case: when compared with the WHO's risk tables the trend \"higher age, more cases of cardio diseases\" is still valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Entries\n",
    "\n",
    "Our dataset is clearly not very clean.\n",
    "If it also contains duplicate rows, they may end up in both the training set and the test set, which might be a problem\n",
    "for estimating the generalization error if they are the result of the poor data collection process.\n",
    "However, with a dataset consisting of 70000 entries some duplicates are to be expected.\n",
    "\n",
    "There appears to be only 41 entries with the exact same feature values - with only 24 of these also matching the same label.\n",
    "We conclude that there is no need for deleting these duplicate rows, because they probably represent real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = plot_df.columns[:-1]\n",
    "\n",
    "duplicates_features = plot_df[plot_df.duplicated(features)]\n",
    "duplicates = plot_df[plot_df.duplicated()]\n",
    "\n",
    "print(f\"Our dataset contains {len(duplicates_features)} duplicates (varying labels)\")\n",
    "print(f\"Our dataset contains {len(duplicates)} duplicates (matching labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up Jupyter Notebook for better performance\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Kernel Logistic Regression\n",
    "\n",
    "#### Formulary\n",
    "Features: $x$\n",
    "\n",
    "Labels: $y$\n",
    "\n",
    "Hypothesis Function: $h(x)$\n",
    "\n",
    "Loss Function: $l(h(x), y)$\n",
    "\n",
    "Regularization Term: $\\Omega(w)$\n",
    "\n",
    "Objective Function: $ J(w) = \\frac{1}{m} \\sum_{i=1}^{m} l(h(x_i), y_i) + \\Omega(w)$\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing corrupted entries\n",
    "\n",
    "As we saw in the exploratory data analysis, our dataset includes corrupted values (e.g. blood pressures above 20000) that need to be expelled before we can make a solid prediction.\n",
    "\n",
    "Therefore, we define reasonable value ranges for the systolic (`ap_hi`)/ diastolic (`ap_lo`) blood pressures,\n",
    "`weight` and `height` features in order to account for any errors during the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_count = len(df)\n",
    "\n",
    "df = df[(50 <= df['ap_lo']) & (df['ap_lo'] <= 150)]\n",
    "df = df[(100 <= df['ap_hi']) & (df['ap_hi'] <= 200)]\n",
    "\n",
    "df = df[(25 <= df['weight']) & (df['weight'] <= 400)]\n",
    "df = df[(100 <= df['height']) & (df['height'] <= 210)]\n",
    "\n",
    "new_count = org_count - len(df)\n",
    "print(f\"{new_count} entries have been excluded due to implausible feature values.\")\n",
    "\n",
    "del new_count, org_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "The information provided by the dataset depends on the category. Some are binary values like `gender`, others are categorical like `cholesterol` or numerical like `weight` and `height`.\n",
    "We need to normalize these data points to a similar scale. This process is called feature scaling.\n",
    "\n",
    "Feature Scaling is necessary because if the range of raw data varies widely, it can be the case that the objective function of some machine learning algorithms will not work properly.\n",
    "This is also the case for the kernel logistic regression.\n",
    "The main reason for this is that the squared exponential kernel algorithm calculates the squared/euclidean distance between the feature points.\n",
    "If one Feature has a broad value range the distance is determined and influenced mostly by this particular feature.\n",
    "\n",
    "Since the Cardiovascular Disease Dataset has a broad value range for example for the Feature `age` (given in days),\n",
    "we standardize our data such that all features have a mean of zero and a standard deviation of 1.\n",
    "We use the standardization formula:\n",
    "\n",
    "$\\tilde{x_i} = \\frac{x_i - \\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(feature):\n",
    "    return (feature-statistics.mean(feature)) / statistics.stdev(feature)\n",
    "\n",
    "df_standardized = pd.DataFrame({})\n",
    "\n",
    "# numerical values\n",
    "df_standardized['age'] = standardize(df['age'])\n",
    "df_standardized['height'] = standardize(df['height'])\n",
    "df_standardized['weight'] = standardize(df['weight'])\n",
    "df_standardized['ap_hi'] = standardize(df['ap_hi'])\n",
    "df_standardized['ap_lo'] = standardize(df['ap_lo'])\n",
    "\n",
    "# binary/categorical values\n",
    "df_standardized['gender'] = df['gender'].apply(lambda t: -1 if t==1 else 1).values\n",
    "df_standardized['smoke'] = df['smoke'].apply(lambda t: -1 if t==0 else 1).values\n",
    "df_standardized['alco'] = df['alco'].apply(lambda t: -1 if t==0 else 1).values\n",
    "df_standardized['active'] = df['active'].apply(lambda t: -1 if t==1 else 1).values\n",
    "df_standardized['cholesterol'] = df['cholesterol'].apply(lambda t: -1 if t==1 else (0 if t==2 else 1)).values\n",
    "df_standardized['gluc'] = df['gluc'].apply(lambda t: -1 if t==1 else (0 if t==2 else 1)).values\n",
    "\n",
    "df_standardized['cardio'] = df['cardio'].apply(lambda t: 1 if t==1 else -1).values\n",
    "\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel / Feature function\n",
    "For the best result we want to include many of our important risk factors into our feature function.\n",
    "\n",
    "Based on our EDA we can say that our data is - as it is - not linear separable.\n",
    "We could introduce extra dimension, find random combination or ask a domain expert which feature combination might be linear separable.\n",
    "This neither fun nor easy, so kernelizing our feature function makes more sense.\n",
    "\n",
    "\n",
    "For the kernel function a popular choice is the squared exponential kernel. This kernel is a universal approximator which is also a good solution for our Project,\n",
    " since it allows us to approximate the unknown feature function without much effort.\n",
    "\n",
    "$h(x) = w^T * \\Phi(x_i)= K(x,z) = exp(\\frac{− ∥x − z∥^2 }{2σ^2})$ with $σ > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the objective function & helper functions\n",
    "\n",
    "#### Squared Exponential Kernel $k(x,z)$\n",
    "$k(x,z) = exp(\\frac{− ∥x − z∥^2 }{2σ^2}) = exp(\\frac{− (x^Tx−2x^Tz+z^Tz)}{2σ^2}) = exp(\\frac{sqdist(x,z)}{2σ^2})$\n",
    "\n",
    "#### Hypothesis Function $h(x)$\n",
    "$h_\\alpha(x) = \\alpha K = \\sum_{j=1}^{m} \\alpha_j k(x_j,x)$\n",
    "\n",
    "#### Loss Function $l(h(x),y)$\n",
    "\n",
    "logistic loss:\n",
    "\n",
    "$l_{logistic}(h_\\alpha(x), y) = log(1 + e^{−y·h_\\alpha(x)})$\n",
    "\n",
    "#### Regularization Term: $\\Omega(w)$\n",
    "\n",
    "$\\Omega(\\alpha) = \\lambda l_2 = \\lambda\\alpha^{\\intercal}K\\alpha$\n",
    "\n",
    "\n",
    "#### Objective Function J\n",
    "\n",
    "kernelized logistic regression:\n",
    "\n",
    "$\n",
    "J(\\alpha)\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} l(h(x_i), y_i) + \\Omega(\\alpha)\n",
    "= \\frac{1}{m} \\sum_{i=1}^m  \\log \\big(1 + \\exp\\big(-y_i \\cdot \\sum_{j=1}^{m} \\alpha_j k(x_j,x_i)\\big) \\big) + \\lambda \\alpha^{\\intercal}K\\alpha\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sqdist(X, Z):\n",
    "    p1 = np.sum(X**2, axis=1)[:, np.newaxis]\n",
    "    p2 = np.sum(Z**2, axis=1)\n",
    "    p3 = -2 * np.dot(X, Z.T)\n",
    "    return p1+p2+p3\n",
    "\n",
    "def sq_exp(X, Z, sigma):\n",
    "    return np.exp(-sqdist(X, Z)/(2*sigma**2) )\n",
    "\n",
    "\n",
    "def J(α, X, y, sigma, lam):\n",
    "    m = X.shape[0]\n",
    "    K = sq_exp(X, X, sigma)\n",
    "\n",
    "    prediction = α @ K\n",
    "    total_loss = sum(np.log(1 + np.exp(-y * prediction)))\n",
    "        \n",
    "    regularization = lam * ((α.T @ K) @ α)\n",
    "    \n",
    "    mean_loss = total_loss / m  + regularization\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the gradient of the regularized kernlized logistic regression objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dJ(α, X, y, sigma, lam):\n",
    "    m = X.shape[0]\n",
    "    K = sq_exp(X, X, sigma)\n",
    "\n",
    "    predictions = α @ K\n",
    "    numerator = (-y * K.T).T\n",
    "    denominator = 1 + np.exp(y * predictions)\n",
    "    gradient = np.sum(numerator / denominator, axis=0)\n",
    "\n",
    "    regularization = 2 * lam * K @ α\n",
    "\n",
    "    mean_gradient = gradient / m + regularization\n",
    "    return mean_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def kernel_lr(X, y, sigma, lam):\n",
    "    # implementation of kernel ridge regression using the scipy optimizer gradient descent\n",
    "    α = np.zeros(X.shape[0],)\n",
    "    α = minimize(J, α, args=(X, y, sigma, lam), jac=dJ, method='CG').x\n",
    "    h = lambda Z: np.dot(α, sq_exp(X, Z, sigma))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in Train / Validation / Test\n",
    "\n",
    "In order to determine the quality of our model, we have to estimate its generalization error (on new data).\n",
    "Therefore, it is common to split the dataset into a training set, and a test set which will be exclusively used for training and testing respectively.\n",
    "\n",
    "However, the Squared Exponential Kernel depends on the parameter $\\sigma$ while the-$\\lambda$ parameter determines how much we regularize our model.\n",
    "First, we need to optimize these parameters by training multiple models with varying ($\\sigma$, $\\lambda$)-pairs.\n",
    "The resulting models need to be compared, but we cannot use our test set for the generalization error, because we would be \"cherry-picking\" the model that best works for the test set instead of real world data.\n",
    "\n",
    "In order to counteract this problem, we use cross-validation for estimating the generalization error.\n",
    "Our training set needs to be split into $k$ folds. We than train $k$ models on $k-1$ training folds und use the last fold for validating our models (each model with a different validation fold).\n",
    "The average over all validation errors serves as a guide for choosing the best parameters $\\sigma$ and $\\lambda$.\n",
    "\n",
    "We will train one last model with optimal parameters on the entire training set, and only than, use the test set for estimating the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_sample_count, shuffle=True):\n",
    "    mask = np.full(data.shape[0], False)\n",
    "    mask[:train_sample_count] = True\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(mask)\n",
    "\n",
    "    #print(np.where(mask == True))\n",
    "\n",
    "    train_data = data[mask]\n",
    "    test_data = data[~mask]\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(data, k=10):\n",
    "    assert k >= 2\n",
    "    datasets = []\n",
    "\n",
    "    if data.shape[0] % k != 0:\n",
    "        print(\"warning: this dataset contains {} entries and cannot be equally divided into {} chunks for cross-validation.\".format(data.shape[0], k))\n",
    "        print(\"Prutruding rows will be dropped.\")\n",
    "        data = data[ : (data.shape[0] // k) * k]\n",
    "\n",
    "    for i in range(k):\n",
    "        data_chunks = np.split(data, k)\n",
    "\n",
    "        val_data = data_chunks.pop(i)\n",
    "        train_data = np.concatenate(data_chunks)\n",
    "        datasets.append((train_data, val_data))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to use 70% of the data for training purposes and 30% for testing.\n",
    "However, training the Kernel Logistic Regression Algorithm for lots of data points and many models is computationally expensive.\n",
    "Unfortunately we already reach our computational limit (in a reasonable amount of time) with ~1500 datapoints.\n",
    "We have an abundantly large dataset and predicting on an already trained model is quite fast.\n",
    "Therefore, our test set will be unusually large compared to our training set.\n",
    "\n",
    "The computational limitations also force us to use a small number of folds for cross-validation. We will only use 2-fold-cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_data = df_standardized.to_numpy()\n",
    "\n",
    "training_samples = 1500\n",
    "fold_count = 2\n",
    "\n",
    "train_data, test_data = train_test_split(cardio_data, training_samples, shuffle=True)\n",
    "cross_val_datasets = cross_val(train_data, k=fold_count)\n",
    "\n",
    "print(f\"training set size: {train_data.shape[0]} ({train_data.shape[0]/fold_count} reserved for validation)\")\n",
    "print(f\"test set size: {test_data.shape[0]} \")\n",
    "\n",
    "del cardio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Choose Features\n",
    "\n",
    "Instead of using all features available we choose to only take features that align with our theoretical background.\n",
    "This means we look at the correlation matrix from the EDA, which results in following claims:\n",
    "\n",
    "#### Good Features:\n",
    "`age`, `weight`, `cholesterol`, `gluc`, `ap_lo`, `ap_hi` and `active` support our hypothesis.\n",
    "\n",
    "As these features a helpful for our model we include these as our features.\n",
    "\n",
    "#### Neutral Features:\n",
    "`gender` doesn't show high enough correlation although it would support our hypothesis.\n",
    "\n",
    "`height` should have no influence on having a cardiovascular disease.\n",
    "\n",
    "We include `gender` in hope that some feature have some hidden correlation that the EDA doesn't show at first glance.\n",
    "As for `height`, we leave it out in favor of faster computing and data variability.\n",
    "\n",
    "#### Bad Features:\n",
    "`alco` and `smoking` normally show a strong correlation towards having a cardiovascular disease.\n",
    "A minor opposite correlation can be found in out dataset.\n",
    "\n",
    "For a better correlation we leave these features out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_labels_and_features(dataset:np.ndarray):\n",
    "    col = {\n",
    "        'age': 0, 'height': 1 , 'weight': 2, 'ap_hi': 3, 'ap_lo': 4, 'gender': 5, 'smoke': 6, \n",
    "        'alco': 7, 'active': 8, 'cholesterol': 9, 'gluc': 10, 'cardio': 11\n",
    "           }\n",
    "    feature_list = [\n",
    "        col['age'], col['weight'], col['cholesterol'],col['gluc'],\n",
    "        col['ap_lo'], col['ap_hi'], col['active'], col['gender']#, col['alco'], col['smoke'], col['height']\n",
    "        ]\n",
    "\n",
    "    labels = dataset[:, col['cardio']]\n",
    "    features = dataset[:, feature_list]\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Optimization and Generalization Error\n",
    "\n",
    "#### Error due to bias:\n",
    "This would occur if we would try to fit our model to non-linear data while keeping a linear model.\n",
    "This naturally can't describe the data correctly since its underfitting.\n",
    "By using a more complex model or in our case, an universal approximator -the squared exponential kernel, we solve this problem.\n",
    "\n",
    "#### Error due to variance:\n",
    "The usage of a kernel is good against underfitting but doesn't help against the opposite problem, overfitting.\n",
    "To reduce the dependency on a given data point and increase the variability of our model we use regularization.\n",
    "\n",
    "Optionally we could reduce our features, but with the current amount of data points that is not necessary.\n",
    "\n",
    "#### Error due to noise:\n",
    "With the help of the exploratory data analysis we found several problems in our dataset.\n",
    "To reduce ambiguity and noise in our data these outliers were removed, and the overall dataset was normalized.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "To get an Idea, which parameters are the best without overfitting, we plot the training and generalization error to estimate the optimal parameters.\n",
    "\n",
    "Since we have no idea how our model perform, we start to train it on different  𝜎,  at first without any regularization.\n",
    "Therefore, we set $\\lambda = 0$ and chose $\\sigma$ to be in an Intervall between 0.5 and 7.5.\n",
    "\n",
    "As a small reference we chose our desired error to be 0.3 since this has been the best result for other classifiers on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(sigmas, lam, train_errors, generalization_errors):\n",
    "    plt.plot(sigmas, generalization_errors, label='generalization error', color='orange')\n",
    "    plt.plot(sigmas, train_errors, label='train error', color='royalblue')\n",
    "    \n",
    "    plt.axhline(0.3, color='tomato', linestyle='--', label=f'Desired Error = {0.3}')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('$\\sigma$')\n",
    "    plt.ylabel('error')\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.title(f'$\\lambda$:{lam}')\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def determine_prediction_error(h, X, y):\n",
    "    predictions = h(X)\n",
    "\n",
    "    wrong_predictions = (predictions*y <= 0).astype(int)\n",
    "    return wrong_predictions.sum()/y.shape[0]\n",
    "\n",
    "\n",
    "def train_models(cross_val_datasets, sigmas, lambdas):\n",
    "    fold_count = len(cross_val_datasets)\n",
    "    model_count = len(sigmas) * len(lambdas) * fold_count\n",
    "    model_number = 0\n",
    "\n",
    "    for i, lam in enumerate(lambdas):\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        for sigma in sigmas:\n",
    "            train_errors.append(0)\n",
    "            val_errors.append(0)\n",
    "            for train_set, val_set in cross_val_datasets:\n",
    "                y_train, X_train = get_labels_and_features(train_set)\n",
    "                y_val, X_val = get_labels_and_features(val_set)\n",
    "\n",
    "                h = kernel_lr(X_train, y_train, sigma=sigma, lam=lam)\n",
    "\n",
    "                train_error = determine_prediction_error(h, X_train, y_train)\n",
    "                val_error = determine_prediction_error(h, X_val, y_val)\n",
    "\n",
    "                # average the train/val errors over all cross_validation sets\n",
    "                train_errors[-1] += train_error / fold_count \n",
    "                val_errors[-1] += val_error / fold_count\n",
    "\n",
    "                model_number += 1\n",
    "                sys.stdout.write(\"\\r\" + f\"Progress: {model_number}/{model_count} models have been trained\") \n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # plot an error graph over all sigmas for a specific lambda \n",
    "        if len(lambdas) > 1:\n",
    "            plt.sca(axs[i])\n",
    "        plot_errors(sigmas, lam, train_errors, val_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1., 2.5, 5.]\n",
    "lambdas = [0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_models(cross_val_datasets, sigmas, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there can be seen there is a huge gap between Train Error and Generalization Error.\n",
    "This means we have a high variance problem, and our model is overfitting.\n",
    "Therefore, we add Regularization to reduce our model complexity.\n",
    "Another possibility would be to add more data, however this is problematic due to the computational time.\n",
    "\n",
    "Let's add a Regularization parameter  𝜆 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1., 2.5, 5.]\n",
    "lambdas = [0.01, 0.1, 1., 5.]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(lambdas), figsize=(15, 5))\n",
    "train_models(cross_val_datasets, sigmas, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the regularization term added we observe way better result with the training & generalisation errors.\n",
    "We now choose the best parameters and calculate the result on the test data set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics\n",
    "\n",
    "For a final evaluation of our model we calculate common metrics for comparability.\n",
    "\n",
    "Our metrics are based around the confusion matrix.\n",
    "\n",
    "True positive = $TP$\n",
    "\n",
    "True negative = $TN$\n",
    "\n",
    "False positive = $FP$\n",
    "\n",
    "False negative = $FN$\n",
    "\n",
    "- Accuracy: the proportion of correct predictions among the total number of predictions made.\n",
    "\n",
    "    $Accuracy = (TP + TN) / (TP + TN + FP + FN) $\n",
    "\n",
    "    So how accurate is our prediction of data points. Note: an overfitted model has a high accuracy.\n",
    "\n",
    "- Precision: the proportion of the true predictions among all the positive predictions made.\n",
    "\n",
    "    $Precision = TP / (TP + FP)$\n",
    "\n",
    "    So how serious do we have to take a positive result. Note: when this score is low, we have lots of false alarms.\n",
    "\n",
    "- Recall: the proportion of the true positive predictions among the total amount of relevant samples in the dataset.\n",
    "\n",
    "    $Recall = TP / (TP + FN)$\n",
    "\n",
    "    So how good is our model in detecting the disease. Note: when this score is low, we missed lots of cases where patients have the disease.\n",
    "\n",
    "- F1: the harmonic mean of the precision and recall.\n",
    "\n",
    "    $F1 = 2 * (precision * recall) / (precision + recall)$\n",
    "\n",
    "    As neither accuracy, precision and recall can judge the model's performance on their own we use the F1 score as a final comparsion value.\n",
    "    This works because the harmonic mean puts the focus on the small values.\n",
    "    So when either precision **or** recall is performing badly it reflects in the F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train hypothesis function on entire training set (including validation set) on best parameters\n",
    "sigma = 1.\n",
    "lam = 1.\n",
    "y_train, X_train = get_labels_and_features(train_data)\n",
    "y_test, X_test = get_labels_and_features(test_data)\n",
    "h = kernel_lr(X_train, y_train, sigma=sigma, lam=lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "X, y =  X_test, y_test\n",
    "\n",
    "decision_boundry = -0.003\n",
    "\n",
    "predictions = h(X)\n",
    "matching_score = (predictions * y > 0).astype(int)\n",
    "\n",
    "# true disease, false alarm, no disease, missed disease\n",
    "tp, fp, tn, fn = [], [], [], []\n",
    "for index, score in enumerate(predictions):\n",
    "    label = y[index]\n",
    "    if score > 0 and label > 0:\n",
    "        tp.append(score)\n",
    "    elif score > 0 and label < 0:\n",
    "        fp.append(score)\n",
    "    elif score <= 0 and label < 0:\n",
    "        tn.append(score)\n",
    "    elif score <= 0 and label > 0:\n",
    "        fn.append(score)\n",
    "\n",
    "\n",
    "accuracy = matching_score.sum() / matching_score.shape[0]\n",
    "precision = len(tp)/ (len(tp)+ len(fp))\n",
    "recall = len(tp) / (len(tp) + len(fn))\n",
    "f_1 = 2 * ( (precision * recall) / (precision + recall) )\n",
    "\n",
    "\n",
    "print('\\t---- Metrics ----\\n'\n",
    "    f'Accuracy:\\t {accuracy}\\n'\n",
    "    f'Precision:\\t {precision}\\n'\n",
    "    f'Recall:\\t\\t {recall}\\n'\n",
    "    f'F1:\\t\\t {f_1}\\n')\n",
    "\n",
    "\n",
    "print('\\t---- Confusion Matrix ----')\n",
    "plot_data = [[len(tn), len(fp)],\n",
    "             [len(fn), len(tp)]]\n",
    "sns.heatmap(plot_data, annot=True, fmt='d')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('actual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f'We predicted correctly that {len(tn)} patients are healthy ({round(100*len(tn) / (len(tn) + len(fp)), 2)}% of all healthy patients).\\n'\n",
    "    f'We predicted correctly that {len(tp)} patients have a cardiovascular disease ({round(100*len(tp) / (len(tp) + len(fn)), 2)}% of all sick patients) .\\n'\n",
    "    f'But we missed {len(fn)} patients ({round(100*len(fn) / (len(tp) + len(fn)), 2)}% of all sick patients) '\n",
    "    f'and gave {len(fp)} false alarms ({round(100*len(fp) / (len(tn) + len(fp)), 2)}% of all healthy patients).\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "fpr, tpr, threshold = metrics.roc_curve(y, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Interpretation & Further Investigation\n",
    "\n",
    "The result of our model are not bad, but also not perfect.\n",
    "For a first prediction the model works pretty good, as in most cases the model correctly predicts the absence or presence of a disease.\n",
    "\n",
    "It is clear that the model performance is better when it comes to false alarms (precision scores), so a doctor should give positive predictions more attention.\n",
    "On the other hand, when the model predicts no disease, and the doctor thinks otherwise its save to trust the doctor, as the recall score of the model could be better.\n",
    "\n",
    "### Additional Bias Term\n",
    "\n",
    "As our goal was to provide a flexible tool for easier medical classification the issue with high recall values,\n",
    "in other words missed cases of a disease, might not be what a doctor wants.\n",
    "It might be way easier to judge a false alarm that it is to find hidden cases of the diease.\n",
    "\n",
    "\n",
    "To provide more flexibility and to tackle this issue we introduce an additional bias value that can influence the proportions of false alarms to missed cases.\n",
    "\n",
    "### Future\n",
    "\n",
    "Our group concluded that a comparison with other algorithms, like the random forrest or k-nearest neighbors algorithm, might be useful to judge the overall performance of our Kernel Logistic Regression algorithm on this dataset\n",
    "\n",
    "TODO: compare our results with others on kaggle -> ca. 70-75 als accuracy/ F1 score....?\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "name": "python391jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}