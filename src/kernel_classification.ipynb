{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kernel Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classifying Cardio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# quick look at data correlation\n",
    "#data = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')\n",
    "#temp_df = pd.concat([data.age, data.weight, data.cardio], axis=1, join='inner')\n",
    "\n",
    "#print(temp_df)\n",
    "\n",
    "#patient_has_cardio = 1\n",
    "#temp_df[\"label\"] = temp_df.apply(lambda row: 1 if (row.cardio == patient_has_cardio) else -1, axis=1)\n",
    "\n",
    "#sns.lmplot(x='weight', y='age', hue='cardio', data=df, fit_reg=False)\n",
    "# sns.lmplot(x='age', y='gender', hue='cardio', data=data, fit_reg=False).set(title='Age / Gender')\n",
    "# sns.lmplot(x='age', y='weight', hue='cardio', data=data, fit_reg=False).set(title='Age / Weight')\n",
    "# sns.lmplot(x='age', y='height', hue='cardio', data=data, fit_reg=False).set(title='Age / Height')\n",
    "# sns.lmplot(x='age', y='cholesterol', hue='cardio', data=data, fit_reg=False).set(title='Age / Cholesterol')\n",
    "# sns.lmplot(x='age', y='gluc', hue='cardio', data=data, fit_reg=False).set(title='Age / Glucose')\n",
    "# sns.lmplot(x='age', y='alco', hue='cardio', data=data, fit_reg=False).set(title='Age / Alcohol')\n",
    "# sns.lmplot(x='age', y='active', hue='cardio', data=data, fit_reg=False).set(title='Age / Active')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO mehr infos plot über daten\n",
    "\n",
    "### Quick Data Inquiry Result\n",
    "> The data is a mess and at the first look not linear separable. So a simple linear classification is not possible.\n",
    ">\n",
    "> A linear regression does not make sense with the result being a binary \"class\".\n",
    "\n",
    "### Kernel / Feature function theory\n",
    "\n",
    "Proceed by using introducing another dimension to make the data linear separable. This is done by using a feature function or kernel.\n",
    "\n",
    "linar: $h(x) = w^T*x$ or nonlinar: $h(x) = w^T*\\Phi(x)$\n",
    "\n",
    "Feature Function $\\Phi(x)$  based on Age = $A$ and Weight = $W$\n",
    "\n",
    "$x = [A, W] ∈ R^2$\n",
    "\n",
    "$ \\Phi(x_2) = [1, A, W, A^2, W^2, AW] ∈ R^6$ - optional with ($\\sqrt{2}$ for ease of calculation)\n",
    "\n",
    "\n",
    " With\n",
    " $ K(x, z) = \\Phi(x)^T\\Phi(z) = (x^T*z+1)^d$ with degree $d = 2$ for a two-dimensional input $x ∈ R^2$.\n",
    "\n",
    " $h(x) = w^T * \\Phi(x_i)= w^T*K(x,z) = w^T * (x^T*z+1)^d $\n",
    "\n",
    "$ min J(w) = \\frac{1}{m} \\sum_{i=1}^{m} l(h(x_i), y_i) + \\Omega(w)$\n",
    "\n",
    "Kernel can be used not just on age and weight but on all other features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LOAD DATA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "IMPORTING DATA\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-'*30); print(\"IMPORTING DATA\");print('-'*30)\n",
    "# limit dataset to 5000 instances for testing purposes (memory issues)\n",
    "df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')[:3000]\n",
    "# df = pd.read_csv('../resources/cardio_train.csv', sep=';', index_col='id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Scaling\n",
    "Scale data so ...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import statistics\n",
    "# TODO: outlier out , define vailid range, text explain\n",
    "\n",
    "# data will be saved in extra file so we dont need to run this every time\n",
    "feature_scale_flag = True\n",
    "\n",
    "if feature_scale_flag:\n",
    "    # Min/Max Scaling on I=[0,1]: x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "    # df['age_scaled'] = ((df['age'])-min(df['age']))/(max(df['age'])-min(df['age']))\n",
    "    # df['height_scaled'] = ((df['height'])-min(df['height']))/(max(df['height'])-min(df['height']))\n",
    "    # df['weight_scaled'] = ((df['weight'])-min(df['weight']))/(max(df['weight'])-min(df['weight']))\n",
    "    # Standardization: x_standardized = (x - µ) / sigma\n",
    "    df['age_standardized'] = (df['age']-statistics.mean(df['age'])) / statistics.stdev(df['age'])\n",
    "    df['height_standardized'] = (df['height']-statistics.mean(df['height'])) / statistics.stdev(df['height'])\n",
    "    df['weight_standardized'] = (df['weight']-statistics.mean(df['weight'])) / statistics.stdev(df['weight'])\n",
    "\n",
    "    # df['bmi'] = (df['weight'] / ((df['height'] / 100) ** 2)).round(decimals=2)\n",
    "    # df['bmi_high'] = (df['bmi'] >= 30).astype(int)\n",
    "\n",
    "    df['cardio'] = df['cardio'].apply(lambda t: 1 if t==1 else -1).values\n",
    "\n",
    "    # eliminate corrupted Data\n",
    "    # TODO: something is wrong here, as its write arrays instead of values, if fixed, its even fast enough to run it without extra save files\n",
    "    # df['ap_lo_fixed'] = [df['ap_lo'] if 50 < x < 150 else -1 for x in df['ap_lo']]\n",
    "    # df['ap_hi_fixed'] = [df['ap_hi'] if 100 < x < 190 else -1 for x in df['ap_hi']]\n",
    "    # df.to_csv('../resources/feature_scaled_data3.csv', index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement functions\n",
    "\n",
    "### squared exponential kernel $k(x,z)$\n",
    "$k(x,z) = exp(− x^Tx−2x^Tz+z^Tz/ 2σ^2) = exp(sqdist(x,z)/2σ^2)$\n",
    "\n",
    "### hypothesis function $h(x)$\n",
    "$h_\\alpha(x) = \\alpha K = \\sum_{j=1}^{m} \\alpha_j k(x_j,x)$\n",
    "\n",
    "### loss function $l(h(x),y)$\n",
    "\n",
    "logistic loss:\n",
    "$l_{logistic}(h_\\alpha(x), y) = log(1 + e^{−y·h(x)})= log(1 + exp(−y · h_\\alpha(x)))$\n",
    "\n",
    "### $l_2$ regularizer\n",
    "\n",
    "$r = \\lambda l_2 = \\lambda\\alpha^{\\intercal}K\\alpha$\n",
    "\n",
    "\n",
    "### objective function J\n",
    "\n",
    "  kernlized logistic regression\n",
    "\n",
    "reuslting in a regularized kernlized logistic:\n",
    "$\n",
    "J(\\alpha) = \\frac{1}{m}\\sum_{i=1}^m  \\log \\big(1 + \\exp\\big(-y_i \\cdot \\sum_{j=1}^{m} \\alpha_j k(x_j,x_i)\\big) \\big) + \\lambda \\alpha^{\\intercal}K\\alpha\n",
    "$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def sqdist(X, Z):\n",
    "    p1 = np.sum(X**2, axis=1)[:, np.newaxis]\n",
    "    p2 = np.sum(Z**2, axis=1)\n",
    "    p3 = -2 * np.dot(X, Z.T)\n",
    "    return p1+p2+p3\n",
    "\n",
    "def sq_exp(X, Z, sigma):\n",
    "    return np.exp(-sqdist(X, Z)/(2*sigma**2) )\n",
    "\n",
    "\n",
    "def J(α, X, y, sigma, lam):\n",
    "    K = sq_exp(X, X, sigma)\n",
    "    m = X.shape[0]\n",
    "    total_loss = 0\n",
    "    regularization = lam * np.dot(np.dot(np.transpose(α), K), α)\n",
    "\n",
    "    for i in range(m):\n",
    "        prediction = 0\n",
    "        for j in range(m):\n",
    "            prediction += α[j]*K[i][j]\n",
    "        logistic_loss = np.log(1 + np.exp(-y[i] * prediction))\n",
    "        total_loss += logistic_loss\n",
    "\n",
    "    mean_loss = total_loss / m  + regularization\n",
    "    return mean_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement the gradient of the regularized kernlized logistic regression objective."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def dJ(α, X, y, sigma, lam):\n",
    "    K = sq_exp(X, X, sigma)\n",
    "    m = X.shape[0]\n",
    "    gradient = 0\n",
    "    regularization = 2*lam * np.dot( K, α)\n",
    "\n",
    "    for i in range(m):\n",
    "        prediction = 0\n",
    "        for j in range(m):\n",
    "            prediction += α[j]*K[i][j]\n",
    "\n",
    "        numerator = -y[i] * K[i]\n",
    "        denominator = 1 + np.exp(y[i] * prediction)\n",
    "        gradient += numerator / denominator\n",
    "\n",
    "    mean_gradient = gradient / m + regularization\n",
    "    return mean_gradient\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def kernel_lr(X, y, sigma, lam):\n",
    "    # implementation of kernel ridge regression using the scipy optimizer gradient descent\n",
    "    α = np.zeros(X.shape[0],)\n",
    "    α = minimize(J, α, args=(X, y, sigma, lam), jac=dJ, method='CG').x\n",
    "    h = lambda Z: np.dot(α, sq_exp(X, Z, sigma))\n",
    "    return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "## Split data in Train / Validation / Test\n",
    "\n",
    "We'll first split our data into a Train set (70%) and Test set (30%).  \n",
    "The training set will be further processed using 10-fold-cross-validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_proportion=0.7, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(data.shape[0])\n",
    "    else:\n",
    "        indices = np.arange(data.shape[0])\n",
    "\n",
    "    split_index = int(train_proportion * data.shape[0])\n",
    "    training_idx = indices[:split_index]\n",
    "    test_idx = indices[split_index:]\n",
    "\n",
    "    return data[training_idx, :], data[test_idx, :]\n",
    "\n",
    "\n",
    "def cross_val(data, k=10):\n",
    "    assert k >= 2\n",
    "    datasets = []\n",
    "\n",
    "    if data.shape[0] % k != 0:\n",
    "        print(\"warning: this dataset contains {} entries and cannot be equally divided into {} chunks for cross-validation.\".format(data.shape[0], k))\n",
    "        print(\"Prutruding rows will be dropped.\")\n",
    "        data = data[ : (data.shape[0] // k) * k]\n",
    "\n",
    "    for i in range(k):\n",
    "        data_chunks = np.split(data, k)\n",
    "\n",
    "        val_data = data_chunks.pop(i)\n",
    "        train_data = np.concatenate(data_chunks)\n",
    "        datasets.append((train_data, val_data))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def get_labels_and_features(dataset:np.ndarray)->Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return labels and features from a given dataset.\n",
    "    :return: labels, features\n",
    "    \"\"\"\n",
    "    # [0] age - [10] active\n",
    "    # [11] cardio\n",
    "    # [12] scaled age - [22]...\n",
    "    raw_features, labels, features = np.hsplit(dataset, [11,12])\n",
    "    return labels.flatten(), features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_data = df.to_numpy()\n",
    "train_data, test_data = train_test_split(cardio_data, shuffle=False)\n",
    "datasets = cross_val(train_data, k=2)\n",
    "\n",
    "# only use a single dataset for now\n",
    "#train_set, val_set = datasets[0]\n",
    "#y, X = get_labels_and_features(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate minimized hypothesis function $h$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average model accuracy for sigma=0.5, lambda=0.1\n",
      "train: 0.7619047619047619\n",
      "val: 0.5752380952380953\n",
      "\n",
      "Average model accuracy for sigma=0.5, lambda=1.0\n",
      "train: 0.7614285714285713\n",
      "val: 0.5761904761904761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def score(h, X, y):\n",
    "    predictions = h(X)\n",
    "    #print(predictions)\n",
    "\n",
    "    score = (predictions*y >= 0).astype(int)\n",
    "    return score.sum()/score.shape[0]\n",
    "\n",
    "def train_n_score(datasets, sigma, lam):\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "    for train_set, val_set in datasets:\n",
    "        y_train, X_train = get_labels_and_features(train_set)\n",
    "        y_val, X_val = get_labels_and_features(val_set)\n",
    "\n",
    "        h = kernel_lr(X_train, y_train, sigma=sigma, lam=lam)\n",
    "\n",
    "        train_accuracy.append(score(h, X_train, y_train))\n",
    "        val_accuracy.append(score(h, X_val, y_val))\n",
    "\n",
    "    print(f'Average model accuracy for sigma={sigma}, lambda={lam}')\n",
    "    print(f'train: {sum(train_accuracy)/len(train_accuracy)}')\n",
    "    print(f'val: {sum(val_accuracy)/len(val_accuracy)}\\n')\n",
    "\n",
    "\n",
    "sigmas=[0.5, 1.]\n",
    "lambdas=[0.1, 1.]\n",
    "\n",
    "for sigma in sigmas:\n",
    "    for lam in lambdas:\n",
    "        train_n_score(datasets, sigma, lam)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: measures against over and underfitting\n",
    "\n",
    "TODO: interpretation\n",
    "\n",
    "TODO: Ausblick -> z.B. andere Lern algorithm, ensemble Methoden wie adaboost, ...\n",
    "\n",
    "TODO: Metrics ->Accuracy, F1 score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}